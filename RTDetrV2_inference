# Jupyter-ready RTDetrV2 batch inference with inline visualization & metrics
# - No CLI args: set variables below
# - Avoids caas_jupyter_tools (uses IPython.display instead)
# - Uses Transformers pipeline("object-detection")
# - Shows annotated images inline (no disk writes)
# - Prints summary metrics and displays DataFrames
# - Measures throughput/latency
#
# Requirements: transformers >= 4.40, torch, pillow, matplotlib, pandas, tqdm
# If not installed in your kernel: `pip install transformers torch pillow matplotlib pandas tqdm`

from pathlib import Path
from typing import List, Dict, Any, Iterable, Tuple
import time
import json

import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from tqdm import tqdm

# Pretty display without external helpers
from IPython.display import display, HTML

# =========================
# Configuration (edit here)
# =========================
MODEL_DIR = Path("/path/to/your_finetuned_model_dir")  # folder with config.json, model.safetensors, preprocessor_config.json
IMAGE_DIR = Path("/path/to/images")                    # directory to scan for images (recursively)
THRESHOLD = 0.25                                       # confidence score threshold
BATCH_SIZE = 8                                         # images per batch
DEVICE = "auto"                                        # "auto", -1 for CPU, or CUDA index like 0, 1, ...
FP16 = True                                            # enable autocast on CUDA
DISPLAY_MAX = 12                                       # max annotated images to display
PRINT_JSON_SAMPLE = 2                                  # print raw detections for first N images for inspection

# =====================================
# Helpers
# =====================================
IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}

def find_images(image_dir: Path) -> List[Path]:
    return [p for p in sorted(image_dir.rglob("*")) if p.suffix.lower() in IMG_EXTS]

def chunked(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

def pil_loader(paths: List[Path]) -> List[Image.Image]:
    imgs = []
    for p in paths:
        try:
            imgs.append(Image.open(p).convert("RGB"))
        except Exception as e:
            print(f"[WARN] Failed to open image {p}: {e}")
            imgs.append(None)
    return imgs

def resolve_device(dev_choice: str, torch_available: bool) -> int:
    if dev_choice == "auto":
        return 0 if torch_available else -1
    try:
        return int(dev_choice)
    except Exception:
        raise ValueError(f'Invalid DEVICE "{dev_choice}" — use "auto", -1, or int like 0')

def annotate_ax(ax, dets: List[Dict[str, Any]], img_w: int, img_h: int, score_thr: float):
    # Do not specify colors per instructions (use matplotlib defaults)
    for d in dets:
        score = float(d.get("score", 0.0))
        if score < score_thr:
            continue
        box = d.get("box", {})
        xmin = float(box.get("xmin", 0.0))
        ymin = float(box.get("ymin", 0.0))
        xmax = float(box.get("xmax", 0.0))
        ymax = float(box.get("ymax", 0.0))
        # Clamp to image bounds
        xmin = max(0.0, min(xmin, img_w - 1))
        ymin = max(0.0, min(ymin, img_h - 1))
        xmax = max(0.0, min(xmax, img_w - 1))
        ymax = max(0.0, min(ymax, img_h - 1))
        w = max(0.0, xmax - xmin)
        h = max(0.0, ymax - ymin)
        rect = Rectangle((xmin, ymin), w, h, fill=False, linewidth=1.5)
        ax.add_patch(rect)
        label = f"{d.get('label','')} {score:.2f}"
        ax.text(xmin, max(0, ymin - 3), label, fontsize=8, bbox=dict(boxstyle="round", fc="w", ec="none", alpha=0.6))

# =====================================
# Safe dependency checks
# =====================================
missing = []
try:
    import torch
    torch_available = True
except Exception:
    torch_available = False
    missing.append("torch")

try:
    from transformers import pipeline
    transformers_available = True
except Exception:
    transformers_available = False
    missing.append("transformers")

if missing:
    print("[SETUP] Missing packages:", ", ".join(missing))
    print("\nInstall in your notebook kernel, e.g.:")
    if "torch" in missing:
        print("pip install --upgrade torch  # pick CUDA/CPU wheels appropriate for your machine")
    if "transformers" in missing:
        print("pip install --upgrade 'transformers>=4.40' pillow matplotlib pandas tqdm")
    print("\nRe-run this cell after installing. No further actions were executed.")
else:
    # =====================================
    # Load resources
    # =====================================
    assert MODEL_DIR.exists(), f"MODEL_DIR does not exist: {MODEL_DIR}"
    assert IMAGE_DIR.exists(), f"IMAGE_DIR does not exist: {IMAGE_DIR}"

    dev_index = resolve_device(DEVICE, torch_available)

    objdet = pipeline(
        task="object-detection",
        model=str(MODEL_DIR),
        device=dev_index,
    )

    img_paths: List[Path] = find_images(IMAGE_DIR)
    assert len(img_paths) > 0, f"No images found under {IMAGE_DIR}"

    print(f"[INFO] Found {len(img_paths)} image(s). Using device: {'CPU' if dev_index==-1 else f'CUDA:{dev_index}'}; FP16={FP16}")

    # =====================================
    # Inference loop
    # =====================================
    detections_all: List[Dict[str, Any]] = []   # flat list of detections (one row per detection)
    per_image_rows: List[Dict[str, Any]] = []    # summary row per image
    autocast_enabled = (FP16 and dev_index != -1 and torch.cuda.is_available())
    amp_dtype = torch.float16 if autocast_enabled else None

    total_start = time.perf_counter()
    n_processed = 0

    for batch_paths in tqdm(list(chunked(img_paths, BATCH_SIZE)), desc="Batches"):
        batch_imgs = pil_loader(batch_paths)
        valid_pairs = [(p, im) for p, im in zip(batch_paths, batch_imgs) if im is not None]
        if not valid_pairs:
            continue

        images = [im for _, im in valid_pairs]

        start = time.perf_counter()
        with torch.cuda.amp.autocast(enabled=autocast_enabled, dtype=amp_dtype) if autocast_enabled else torch.no_grad():
            dets_list = objdet(images, threshold=THRESHOLD, batch_size=len(images))
        end = time.perf_counter()
        batch_time = end - start
        per_image_time = batch_time / max(1, len(images))

        # Aggregate
        for (p, im), dets in zip(valid_pairs, dets_list):
            w, h = im.size
            # filter again for safety
            dets = [d for d in dets if float(d.get("score", 0.0)) >= THRESHOLD]
            for d in dets:
                box = d.get("box", {})
                xmin = float(box.get("xmin", 0.0))
                ymin = float(box.get("ymin", 0.0))
                xmax = float(box.get("xmax", 0.0))
                ymax = float(box.get("ymax", 0.0))
                area = max(0.0, (xmax - xmin)) * max(0.0, (ymax - ymin))
                detections_all.append({
                    "image_path": str(p),
                    "label": d.get("label", ""),
                    "score": float(d.get("score", 0.0)),
                    "xmin": xmin, "ymin": ymin, "xmax": xmax, "ymax": ymax,
                    "width": w, "height": h, "area_px": area,
                    "inf_time_s": per_image_time,
                })
            per_image_rows.append({
                "image_path": str(p),
                "width": w, "height": h,
                "num_dets": len(dets),
                "avg_score": float(np.mean([dd["score"] for dd in dets])) if dets else 0.0,
                "latency_s": per_image_time,
            })
            n_processed += 1

    total_end = time.perf_counter()
    total_time = total_end - total_start
    ips = n_processed / total_time if total_time > 0 else float("inf")

    # =====================================
    # Metrics & DataFrames
    # =====================================
    per_image_df = pd.DataFrame(per_image_rows)
    dets_df = pd.DataFrame(detections_all)

    print("\n=== Summary Metrics ===")
    print(f"Images processed: {n_processed}")
    print(f"Total time (s):  {total_time:.4f}")
    print(f"Images/sec:      {ips:.2f}")
    if len(per_image_df):
        print(f"Median latency (s): {per_image_df['latency_s'].median():.4f}")
        print(f"Mean detections / image: {per_image_df['num_dets'].mean():.2f}")

    # Per-class distribution and score stats
    if len(dets_df):
        per_class = dets_df.groupby("label").agg(
            detections=("label", "count"),
            mean_score=("score", "mean"),
            median_area_px=("area_px", "median"),
        ).reset_index().sort_values("detections", ascending=False)
    else:
        per_class = pd.DataFrame(columns=["label", "detections", "mean_score", "median_area_px"])

    # Top images by detections
    if len(per_image_df):
        top_images = per_image_df.sort_values("num_dets", ascending=False).head(10)
    else:
        top_images = pd.DataFrame(columns=["image_path","num_dets","avg_score","latency_s","width","height"])

    # Display DataFrames nicely
    display(HTML("<h3>Per-image summary (latency & counts)</h3>"))
    display(per_image_df)
    display(HTML("<h3>Per-class distribution</h3>"))
    display(per_class)
    display(HTML("<h3>Top 10 images by detection count</h3>"))
    display(top_images)

    # Print a compact JSON-like preview for first N images
    if PRINT_JSON_SAMPLE > 0 and len(dets_df):
        print("\n=== Sample detections (first images) ===")
        for ipath in per_image_df["image_path"].head(PRINT_JSON_SAMPLE).tolist():
            rows = dets_df[dets_df["image_path"] == ipath]
            sample = []
            for _, r in rows.iterrows():
                sample.append({
                    "label": r["label"],
                    "score": float(r["score"]),
                    "box": {"xmin": float(r["xmin"]), "ymin": float(r["ymin"]), "xmax": float(r["xmax"]), "ymax": float(r["ymax"])}
                })
            print(f"- {ipath}")
            print(json.dumps(sample, ensure_ascii=False, indent=2))

    # =====================================
    # Visualization (annotated images inline)
    # One figure per image (each chart its own plot; no color styles)
    # =====================================
    to_show = per_image_df["image_path"].tolist()[:DISPLAY_MAX] if len(per_image_df) else []
    for ipath in to_show:
        # Reload image (ensure fresh read & correct size)
        im = Image.open(ipath).convert("RGB")
        w, h = im.size
        rows = dets_df[dets_df["image_path"] == ipath].to_dict(orient="records")

        plt.figure(figsize=(10, 8))
        plt.imshow(im)
        ax = plt.gca()
        ax.set_axis_off()
        annotate_ax(ax, rows, w, h, THRESHOLD)
        title = f"{Path(ipath).name} — dets: {len(rows)}"
        plt.title(title)
        plt.show()

    print("\n[Done] All results shown inline (no files written).")
