import os
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from transformers import DetrForObjectDetection, DetrImageProcessor
from PIL import Image

# 1. 사용자 정의 COCO 스타일 Dataset 클래스
class COCODetectionDataset(Dataset):
    def __init__(self, annotations, image_dir, processor, transforms=None):
        """
        annotations: 리스트 형태의 COCO annotation dicts
        image_dir: 이미지 파일이 저장된 디렉터리
        processor: DetrImageProcessor 인스턴스
        transforms: 토치비전 transforms (선택적)
        """
        self.annotations = annotations
        self.image_dir = image_dir
        self.processor = processor
        self.transforms = transforms

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        ann = self.annotations[idx]
        # 이미지 로드 및 RGB 변환
        img_path = os.path.join(self.image_dir, ann['file_name'])
        image = Image.open(img_path).convert('RGB')
        if self.transforms:
            image = self.transforms(image)

        # processor를 이용해 모델 입력과 라벨 인코딩
        encoding = self.processor(images=image,
                                  annotations=ann,
                                  return_tensors="pt")
        # 배치 차원을 제거하고 텐서 딕셔너리 반환
        item = {k: v.squeeze() for k, v in encoding.items()}
        return item

# 2. collate_fn: 배치 패딩 및 텐서 합치기
def collate_fn(batch):
    return processor.pad(batch, return_tensors="pt")

# 3. 하이퍼파라미터 및 객체 초기화
device_lr = 1e-4
batch_size = 2
num_epochs = 5

# Accelerator 설정
accelerator = Accelerator()
device = accelerator.device

# DETR 모델 & 프로세서 불러오기
processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = DetrForObjectDetection.from_pretrained(
    "facebook/detr-resnet-50"
)
model.to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=device_lr)

# 4. 데이터셋 및 DataLoader 생성
# (여기서는 예시로 annotations_train, annotations_val, image_dir 사용 가정)
train_dataset = COCODetectionDataset(
    annotations=annotations_train,
    image_dir=image_dir,
    processor=processor,
    transforms=None
)
val_dataset = COCODetectionDataset(
    annotations=annotations_val,
    image_dir=image_dir,
    processor=processor,
    transforms=None
)

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_fn
)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    collate_fn=collate_fn
)

# 5. Accelerator로 준비
train_loader, val_loader, model, optimizer = accelerator.prepare(
    train_loader, val_loader, model, optimizer
)

# 6. 학습 & 평가 함수

def train_one_epoch(model, loader, optimizer, device):
    model.train()
    total_loss = 0.0
    for batch in loader:
        # 입력 데이터
        pixel_values = batch['pixel_values'].to(device)
        labels = [{k: v.to(device) for k, v in t.items()} for t in batch['labels']]

        # 순전파 및 손실
        outputs = model(pixel_values=pixel_values, labels=labels)
        loss = outputs.loss

        # 역전파 & 최적화
        optimizer.zero_grad()
        accelerator.backward(loss)
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(loader)

@torch.no_grad()
def evaluate(model, loader, device, processor, threshold=0.5):
    model.eval()
    all_results = []
    for batch in loader:
        pixel_values = batch['pixel_values'].to(device)
        outputs = model(pixel_values=pixel_values)
        # 후처리
        target_sizes = torch.tensor([ (h, w) for h, w in batch['size'] ])
        results = processor.post_process_object_detection(
            outputs,
            target_sizes=target_sizes,
            threshold=threshold
        )
        all_results.extend(results)
    return all_results

# 7. 메인 학습 루프
for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader, optimizer, device)
    print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}")

    detections = evaluate(model, val_loader, device, processor)
    # (val annotations과 비교해 mAP 등 평가 지표 계산 가능)
    print(f"Epoch {epoch+1}, Validation detections: {len(detections)} items")

# 8. 모델 저장
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
torch.save(unwrapped_model.state_dict(), "detr_finetuned.pth")
