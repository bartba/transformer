# =========================================
# Dockerfile (train-rtdetrv2)
# =========================================
FROM huggingface/transformers-pytorch-gpu

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    git sudo nano vim wget curl unzip ffmpeg libgl1 python-is-python3 \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Python packages for training & notebooks
# - transformers/accelerate: HF training stack
# - datasets: load COCO or custom sets
# - evaluate: metrics glue
# - pycocotools: COCO metrics
# - albumentations: common augs (optional)
# - jupyter/tensorboard: same as your YOLO workflow
# - fastapi/uvicorn: keep parity; handy for quick eval servers
RUN pip install --no-cache-dir \
    "transformers>=4.43.0" \
    "accelerate>=0.33.0" \
    "datasets>=2.20.0" \
    "evaluate>=0.4.2" \
    "pycocotools>=2.0.7" \
    "albumentations>=1.4.8" \
    "opencv-python" \
    "numpy" "pandas" "matplotlib" \
    "tensorboard" \
    "jupyter" "notebook" \
    "fastapi" "uvicorn[standard]"

# (Optional) bitsandbytes for 8-bit optimizers on large models
# RUN pip install --no-cache-dir bitsandbytes

# Workdir & ports
WORKDIR /workspace
EXPOSE 8888 6006

# Place training scripts (see below)
# You can COPY your project files here; leaving commented so you can bind-mount instead.
# COPY ./train /workspace/train

# Jupyter (no token) like your YOLOv12 image
CMD ["bash", "-lc", "jupyter notebook --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.quit_button=False"]







# 1) Build
docker build -t train-rtdetrv2 -f Dockerfile .

# 2) Run (mount your dataset & logs)
docker run --gpus all -it --rm \
  -p 8888:8888 -p 6006:6006 \
  -v /path/to/your/dataset:/data \
  -v $(pwd)/outputs:/workspace/outputs \
  -v $(pwd)/scripts:/workspace/scripts \
  --shm-size=1g \
  --name rtdetr_train \
  train-rtdetrv2

# 3) In container, launch training
python /workspace/scripts/train_rtdetrv2.py \
  --any-args  # (script uses env vars; set MODEL_ID, DATA_DIR, etc. via -e on docker run)

# Example with env overrides:
# docker run ... -e MODEL_ID="PaddlePaddle/rt-detrv2-r18" -e BATCH=8 -e EPOCHS=100 ...




# JUPYTER CELL 2 — Global config (easy to tweak in notebook)
CFG = {
    "model_id": "PaddlePaddle/rt-detrv2-r50",   # <-- Swap to Deformable DETR below if desired
    "data_root": "/data",                       # mount path
    "image_size": 640,                          # shortest edge resize
    "epochs": 50,
    "train_batch": 4,
    "eval_batch": 4,
    "lr": 5e-5,
    "num_workers": 8,
    "seed": 42,
    "output_dir": "/workspace/outputs/rtdetrv2_notebook",
    "eval_interval": "epoch",                  # "epoch" or "steps"
    "logging_steps": 50,
    "save_total_limit": 2,
    "fp16": True,
    "warmup_ratio": 0.05,
    "gradient_accumulation_steps": 1,
    "weight_decay": 0.05,
    "max_grad_norm": 1.0,
    "resume_from_checkpoint": None,            # or path to checkpoint
    "conf_threshold": 0.001,                   # low threshold for COCO eval, NMS happens inside postprocess
    "iou_threshold": 0.5,                      # for visualization (not COCO mAP which sweeps 0.5:0.95)
}
set_seed(CFG["seed"])
Path(CFG["output_dir"]).mkdir(parents=True, exist_ok=True)

# Derived paths
TRAIN_IMG_DIR = f"{CFG['data_root']}/images/train"
VAL_IMG_DIR   = f"{CFG['data_root']}/images/val"
TRAIN_JSON    = f"{CFG['data_root']}/annotations/instances_train.json"
VAL_JSON      = f"{CFG['data_root']}/annotations/instances_val.json"

assert Path(TRAIN_JSON).exists() and Path(VAL_JSON).exists(), "COCO JSON not found."








# JUPYTER CELL 4 — Dataset
class CocoDetectionDataset(Dataset):
    """
    Returns raw PIL image and COCO-style target dict.
    We transform to model inputs in a collate_fn (keeps flexibility).
    """
    def __init__(self, img_dir: str, ann_file: str, transforms: A.BasicTransform | None = None):
        self.coco = COCO(ann_file)
        self.img_dir = Path(img_dir)
        self.ids = list(self.coco.imgs.keys())
        self.transforms = transforms

        # build category mappings
        cats = self.coco.loadCats(self.coco.getCatIds())
        self.catid2contig = {cat["id"]: i for i, cat in enumerate(sorted(cats, key=lambda x: x["id"]))}
        self.id2label = {i: cat["name"] for i, cat in enumerate(sorted(cats, key=lambda x: x["id"]))}
        self.label2id = {v: k for k, v in self.id2label.items()}

    def __len__(self): return len(self.ids)

    def _load_image(self, img_info):
        path = self.img_dir / img_info["file_name"]
        img = Image.open(path).convert("RGB")
        return img

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_info = self.coco.loadImgs([img_id])[0]
        ann_ids = self.coco.getAnnIds(imgIds=[img_id], iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        # build target
        boxes, classes, area, iscrowd = [], [], [], []
        for a in anns:
            # Skip degenerate boxes
            x, y, w, h = a["bbox"]
            if w <= 1 or h <= 1:
                continue
            boxes.append([x, y, w, h])  # COCO xywh
            classes.append(self.catid2contig[a["category_id"]])
            area.append(a.get("area", w * h))
            iscrowd.append(int(a.get("iscrowd", 0)))

        img = self._load_image(img_info)

        # Albumentations (optional, applied on np array)
        if self.transforms is not None:
            img_np = np.array(img)
            # Convert boxes to albumentations format
            b = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), np.float32)
            labels = classes
            transformed = self.transforms(
                image=img_np,
                bboxes=b.tolist(),
                class_labels=labels
            )
            img = Image.fromarray(transformed["image"])
            boxes = transformed["bboxes"]
            classes = transformed["class_labels"]

        target = {
            "image_id": img_id,
            "boxes": boxes,            # xywh
            "classes": classes,        # contiguous ids
            "area": area,
            "iscrowd": iscrowd,
            "orig_size": [img_info["height"], img_info["width"]],
            "size": [img.size[1], img.size[0]],
        }
        return img, target

# Light/default augmentations (feel free to extend)
train_tfms = A.Compose([
    A.LongestMaxSize(max_size=CFG["image_size"]),
    A.PadIfNeeded(CFG["image_size"], CFG["image_size"], border_mode=cv2.BORDER_CONSTANT),
    A.HorizontalFlip(p=0.5),
], bbox_params=A.BboxParams(format="coco", label_fields=["class_labels"]))

val_tfms = A.Compose([
    A.LongestMaxSize(max_size=CFG["image_size"]),
    A.PadIfNeeded(CFG["image_size"], CFG["image_size"], border_mode=cv2.BORDER_CONSTANT),
], bbox_params=A.BboxParams(format="coco", label_fields=["class_labels"]))

train_ds = CocoDetectionDataset(TRAIN_IMG_DIR, TRAIN_JSON, transforms=train_tfms)
val_ds   = CocoDetectionDataset(VAL_IMG_DIR,   VAL_JSON,   transforms=val_tfms)

# Propagate labels into model config (nice to have)
model.config.id2label = train_ds.id2label
model.config.label2id = {v: k for k, v in train_ds.id2label.items()}
len(train_ds), len(val_ds), train_ds.id2label











#===================
# train_rtdetrv2.py
#===================
import os, json, torch
from datasets import load_dataset
from transformers import AutoImageProcessor, AutoModelForObjectDetection, Trainer, TrainingArguments, DefaultDataCollator

# === Config ===
MODEL_ID = os.getenv("MODEL_ID", "PaddlePaddle/rt-detrv2-r50")  # Example; pick a size that fits your GPU
DATA_DIR = os.getenv("DATA_DIR", "/data")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/workspace/outputs/rtdetrv2")
IMAGE_SIZE = int(os.getenv("IMAGE_SIZE", "640"))
NUM_WORKERS = int(os.getenv("NUM_WORKERS", "8"))

# === Load processor/model ===
processor = AutoImageProcessor.from_pretrained(MODEL_ID)
model = AutoModelForObjectDetection.from_pretrained(
    MODEL_ID,
    ignore_mismatched_sizes=True,  # helpful when class count differs from pretrain
)

# === Dataset: COCO format via HF datasets ===
# Expect files under: /data/images/{train,val} and /data/annotations/instances_{train,val}.json
def coco_loader(split):
    return load_dataset(
        "coco",
        data_dir=DATA_DIR,
        split=split,
        # if local COCO, we point to images/ and annotations/ under DATA_DIR
    )

train_ds = coco_loader("train")
val_ds   = coco_loader("validation")

# === Preprocess ===
id2label = {i: c for i, c in enumerate(train_ds.features["objects"].feature["category"].names)}
label2id = {v: k for k, v in id2label.items()}
model.config.id2label = id2label
model.config.label2id = label2id

def transform(example):
    images = [x for x in example["image"]]
    targets = []
    for anns in example["objects"]:
        boxes = anns["bbox"]      # [x, y, w, h]
        area  = anns["area"]
        cats  = anns["category"]
        iscrowd = anns["is_crowd"]
        targets.append({
            "boxes": boxes,
            "classes": cats,
            "area": area,
            "iscrowd": iscrowd,
        })
    proc = processor(
        images,
        annotations=targets,
        return_tensors="pt",
        size={"shortest_edge": IMAGE_SIZE},  # RT-DETR uses resize w/ keep aspect
    )
    return proc

train_ds = train_ds.with_transform(transform)
val_ds   = val_ds.with_transform(transform)

# === Training ===
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=int(os.getenv("BATCH", "2")),
    per_device_eval_batch_size=int(os.getenv("BATCH_EVAL", "2")),
    dataloader_num_workers=NUM_WORKERS,
    learning_rate=float(os.getenv("LR", "5e-5")),
    num_train_epochs=int(os.getenv("EPOCHS", "50")),
    fp16=torch.cuda.is_available(),
    save_strategy="epoch",
    evaluation_strategy="epoch",
    logging_steps=50,
    report_to=["tensorboard"],
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    data_collator=DefaultDataCollator(),
)

trainer.train()
trainer.save_model(OUTPUT_DIR)
processor.save_pretrained(OUTPUT_DIR)

# Swap to Deformable DETR by changing just two lines:
# MODEL_ID = "SenseTime/deformable-detr-single-scale-dc5"  # or another HF ckpt
# from transformers import DeformableDetrImageProcessor as AutoImageProcessor, DeformableDetrForObjectDetection as AutoModelForObjectDetection








