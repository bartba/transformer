# CELL 4
from transformers import DefaultDataCollator

def collate_processor(batch):
    images, targets = list(zip(*batch))

    anno_list = []
    for t in targets:
        boxes = t["boxes"]
        classes = t["classes"]
        if len(boxes) == 0:
            anno_list.append({"boxes": [], "classes": [], "area": [], "iscrowd": []})
            continue
        anno_list.append({
            "boxes": boxes,           # xywh (COCO)
            "classes": classes,       # contiguous ids
            "area": t["area"] if len(t["area"]) == len(boxes) else [w*h for (x,y,w,h) in boxes],
            "iscrowd": t["iscrowd"],
        })

    enc = processor(
        list(images),
        annotations=anno_list,
        return_tensors="pt",
        size={"shortest_edge": CFG["image_size"]},
    )
    return enc

data_collator = DefaultDataCollator()




# CELL 5
train_loader = DataLoader(train_ds, batch_size=CFG["train_batch"], shuffle=True,
                          num_workers=CFG["num_workers"], collate_fn=collate_processor)
val_loader   = DataLoader(val_ds,   batch_size=CFG["eval_batch"], shuffle=False,
                          num_workers=CFG["num_workers"], collate_fn=collate_processor)

b = next(iter(train_loader))
{k: v.shape for k, v in b.items() if isinstance(v, torch.Tensor)}







# CELL 6
val_coco = COCO(VAL_JSON)

def _to_coco_predictions(processor, outputs, pixel_sizes, image_ids):
    results = []
    processed = processor.post_process_object_detection(
        outputs,
        threshold=CFG["conf_threshold"],
        target_sizes=pixel_sizes,   # (H, W)
    )
    # contiguous id -> 원래 COCO category_id 매핑 복구
    contig2cat = {v:k for k,v in train_ds.catid2contig.items()}

    for res, img_id in zip(processed, image_ids):
        boxes = res["boxes"].detach().cpu().numpy()      # xyxy
        scores = res["scores"].detach().cpu().numpy()
        labels = res["labels"].detach().cpu().numpy()
        for (x1,y1,x2,y2), s, lab in zip(boxes, scores, labels):
            w, h = x2 - x1, y2 - y1
            results.append({
                "image_id": int(img_id),
                "category_id": int(contig2cat[int(lab)]),
                "bbox": [float(x1), float(y1), float(w), float(h)],
                "score": float(s),
            })
    return results

from transformers.trainer import Trainer

class CocoTrainer(Trainer):
    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix="eval"):
        self.model.eval()
        all_preds = []

        for step, batch in enumerate(dataloader):
            image_ids = [int(x["image_id"]) if "image_id" in x else -1 for x in batch["labels"]]
            sizes     = [x["size"] for x in batch["labels"]]

            with torch.no_grad():
                batch = {k: (v.to(self.args.device) if isinstance(v, torch.Tensor) else v)
                         for k, v in batch.items()}
                outputs = self.model(pixel_values=batch["pixel_values"],
                                     pixel_mask=batch.get("pixel_mask"),
                                     labels=batch["labels"])

                pixel_sizes = torch.tensor([s for s in sizes], device=self.args.device)
                preds = _to_coco_predictions(processor, outputs, pixel_sizes=pixel_sizes, image_ids=image_ids)
                all_preds.extend(preds)

        coco_dt = val_coco.loadRes(all_preds) if len(all_preds) > 0 else None
        if coco_dt is not None:
            coco_eval = COCOeval(val_coco, coco_dt, iouType="bbox")
            coco_eval.evaluate(); coco_eval.accumulate(); coco_eval.summarize()
            metrics = {
                f"{metric_key_prefix}_mAP": coco_eval.stats[0],
                f"{metric_key_prefix}_mAP50": coco_eval.stats[1],
                f"{metric_key_prefix}_mAP75": coco_eval.stats[2],
                f"{metric_key_prefix}_mAP_small": coco_eval.stats[3],
                f"{metric_key_prefix}_mAP_medium": coco_eval.stats[4],
                f"{metric_key_prefix}_mAP_large": coco_eval.stats[5],
                f"{metric_key_prefix}_AR_1": coco_eval.stats[6],
                f"{metric_key_prefix}_AR_10": coco_eval.stats[7],
                f"{metric_key_prefix}_AR_100": coco_eval.stats[8],
                f"{metric_key_prefix}_AR_small": coco_eval.stats[9],
                f"{metric_key_prefix}_AR_medium": coco_eval.stats[10],
                f"{metric_key_prefix}_AR_large": coco_eval.stats[11],
            }
        else:
            metrics = {f"{metric_key_prefix}_mAP": 0.0}

        self.log(metrics)
        return type("EvalLoopOutput", (), dict(
            metrics=metrics, num_samples=len(val_ds),
            predictions=None, label_ids=None, losses=None
        ))()







# CELL 7
args = TrainingArguments(
    output_dir=CFG["output_dir"],
    per_device_train_batch_size=CFG["train_batch"],
    per_device_eval_batch_size=CFG["eval_batch"],
    dataloader_num_workers=CFG["num_workers"],
    learning_rate=CFG["lr"],
    num_train_epochs=CFG["epochs"],
    fp16=CFG["fp16"],
    warmup_ratio=CFG["warmup_ratio"],
    gradient_accumulation_steps=CFG["gradient_accumulation_steps"],
    weight_decay=CFG["weight_decay"],
    logging_steps=CFG["logging_steps"],
    save_strategy="epoch",
    evaluation_strategy=CFG["eval_interval"],
    report_to=["tensorboard"],
    metric_for_best_model="eval_mAP",
    load_best_model_at_end=True,
    save_total_limit=CFG["save_total_limit"],
    max_grad_norm=CFG["max_grad_norm"],
)

trainer = CocoTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    data_collator=data_collator,
)




# CELL 8
if CFG["resume_from_checkpoint"]:
    trainer.train(resume_from_checkpoint=CFG["resume_from_checkpoint"])
else:
    trainer.train()

trainer.save_model(CFG["output_dir"])
processor.save_pretrained(CFG["output_dir"])









# CELL 9
def draw_boxes(image: Image.Image, boxes_xyxy: np.ndarray, labels: List[int], scores: np.ndarray,
               id2label: Dict[int,str], score_thr=0.3):
    img = np.array(image).copy()
    for (x1,y1,x2,y2), lab, s in zip(boxes_xyxy, labels, scores):
        if s < score_thr: continue
        x1,y1,x2,y2 = map(int, [x1,y1,x2,y2])
        cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)
        text = f"{id2label.get(int(lab), str(lab))}:{s:.2f}"
        cv2.putText(img, text, (x1, max(0,y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)
    plt.figure(figsize=(8,8)); plt.imshow(img); plt.axis("off"); plt.show()

idx = random.randint(0, len(val_ds)-1)
img, tgt = val_ds[idx]
inputs = processor([img], return_tensors="pt").to(model.device)
with torch.no_grad():
    out = model(**inputs)

post = processor.post_process_object_detection(
    out, threshold=0.3,
    target_sizes=torch.tensor([[img.size[1], img.size[0]]]).to(model.device)
)[0]

draw_boxes(img,
           post["boxes"].cpu().numpy(),
           post["labels"].cpu().numpy(),
           post["scores"].cpu().numpy(),
           model.config.id2label,
           score_thr=0.3)



