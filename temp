# =========================================
# Dockerfile (train-rtdetrv2)
# =========================================
FROM huggingface/transformers-pytorch-gpu

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    git sudo nano vim wget curl unzip ffmpeg libgl1 python-is-python3 \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Python packages for training & notebooks
# - transformers/accelerate: HF training stack
# - datasets: load COCO or custom sets
# - evaluate: metrics glue
# - pycocotools: COCO metrics
# - albumentations: common augs (optional)
# - jupyter/tensorboard: same as your YOLO workflow
# - fastapi/uvicorn: keep parity; handy for quick eval servers
RUN pip install --no-cache-dir \
    "transformers>=4.43.0" \
    "accelerate>=0.33.0" \
    "datasets>=2.20.0" \
    "evaluate>=0.4.2" \
    "pycocotools>=2.0.7" \
    "albumentations>=1.4.8" \
    "opencv-python" \
    "numpy" "pandas" "matplotlib" \
    "tensorboard" \
    "jupyter" "notebook" \
    "fastapi" "uvicorn[standard]"

# (Optional) bitsandbytes for 8-bit optimizers on large models
# RUN pip install --no-cache-dir bitsandbytes

# Workdir & ports
WORKDIR /workspace
EXPOSE 8888 6006

# Place training scripts (see below)
# You can COPY your project files here; leaving commented so you can bind-mount instead.
# COPY ./train /workspace/train

# Jupyter (no token) like your YOLOv12 image
CMD ["bash", "-lc", "jupyter notebook --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.quit_button=False"]







# 1) Build
docker build -t train-rtdetrv2 -f Dockerfile .

# 2) Run (mount your dataset & logs)
docker run --gpus all -it --rm \
  -p 8888:8888 -p 6006:6006 \
  -v /path/to/your/dataset:/data \
  -v $(pwd)/outputs:/workspace/outputs \
  -v $(pwd)/scripts:/workspace/scripts \
  --shm-size=1g \
  --name rtdetr_train \
  train-rtdetrv2

# 3) In container, launch training
python /workspace/scripts/train_rtdetrv2.py \
  --any-args  # (script uses env vars; set MODEL_ID, DATA_DIR, etc. via -e on docker run)

# Example with env overrides:
# docker run ... -e MODEL_ID="PaddlePaddle/rt-detrv2-r18" -e BATCH=8 -e EPOCHS=100 ...




# JUPYTER CELL 2 â€” Global config (easy to tweak in notebook)
CFG = {
    "model_id": "PaddlePaddle/rt-detrv2-r50",   # <-- Swap to Deformable DETR below if desired
    "data_root": "/data",                       # mount path
    "image_size": 640,                          # shortest edge resize
    "epochs": 50,
    "train_batch": 4,
    "eval_batch": 4,
    "lr": 5e-5,
    "num_workers": 8,
    "seed": 42,
    "output_dir": "/workspace/outputs/rtdetrv2_notebook",
    "eval_interval": "epoch",                  # "epoch" or "steps"
    "logging_steps": 50,
    "save_total_limit": 2,
    "fp16": True,
    "warmup_ratio": 0.05,
    "gradient_accumulation_steps": 1,
    "weight_decay": 0.05,
    "max_grad_norm": 1.0,
    "resume_from_checkpoint": None,            # or path to checkpoint
    "conf_threshold": 0.001,                   # low threshold for COCO eval, NMS happens inside postprocess
    "iou_threshold": 0.5,                      # for visualization (not COCO mAP which sweeps 0.5:0.95)
}
set_seed(CFG["seed"])
Path(CFG["output_dir"]).mkdir(parents=True, exist_ok=True)

# Derived paths
TRAIN_IMG_DIR = f"{CFG['data_root']}/images/train"
VAL_IMG_DIR   = f"{CFG['data_root']}/images/val"
TRAIN_JSON    = f"{CFG['data_root']}/annotations/instances_train.json"
VAL_JSON      = f"{CFG['data_root']}/annotations/instances_val.json"

assert Path(TRAIN_JSON).exists() and Path(VAL_JSON).exists(), "COCO JSON not found."








#===================
# train_rtdetrv2.py
#===================
import os, json, torch
from datasets import load_dataset
from transformers import AutoImageProcessor, AutoModelForObjectDetection, Trainer, TrainingArguments, DefaultDataCollator

# === Config ===
MODEL_ID = os.getenv("MODEL_ID", "PaddlePaddle/rt-detrv2-r50")  # Example; pick a size that fits your GPU
DATA_DIR = os.getenv("DATA_DIR", "/data")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/workspace/outputs/rtdetrv2")
IMAGE_SIZE = int(os.getenv("IMAGE_SIZE", "640"))
NUM_WORKERS = int(os.getenv("NUM_WORKERS", "8"))

# === Load processor/model ===
processor = AutoImageProcessor.from_pretrained(MODEL_ID)
model = AutoModelForObjectDetection.from_pretrained(
    MODEL_ID,
    ignore_mismatched_sizes=True,  # helpful when class count differs from pretrain
)

# === Dataset: COCO format via HF datasets ===
# Expect files under: /data/images/{train,val} and /data/annotations/instances_{train,val}.json
def coco_loader(split):
    return load_dataset(
        "coco",
        data_dir=DATA_DIR,
        split=split,
        # if local COCO, we point to images/ and annotations/ under DATA_DIR
    )

train_ds = coco_loader("train")
val_ds   = coco_loader("validation")

# === Preprocess ===
id2label = {i: c for i, c in enumerate(train_ds.features["objects"].feature["category"].names)}
label2id = {v: k for k, v in id2label.items()}
model.config.id2label = id2label
model.config.label2id = label2id

def transform(example):
    images = [x for x in example["image"]]
    targets = []
    for anns in example["objects"]:
        boxes = anns["bbox"]      # [x, y, w, h]
        area  = anns["area"]
        cats  = anns["category"]
        iscrowd = anns["is_crowd"]
        targets.append({
            "boxes": boxes,
            "classes": cats,
            "area": area,
            "iscrowd": iscrowd,
        })
    proc = processor(
        images,
        annotations=targets,
        return_tensors="pt",
        size={"shortest_edge": IMAGE_SIZE},  # RT-DETR uses resize w/ keep aspect
    )
    return proc

train_ds = train_ds.with_transform(transform)
val_ds   = val_ds.with_transform(transform)

# === Training ===
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=int(os.getenv("BATCH", "2")),
    per_device_eval_batch_size=int(os.getenv("BATCH_EVAL", "2")),
    dataloader_num_workers=NUM_WORKERS,
    learning_rate=float(os.getenv("LR", "5e-5")),
    num_train_epochs=int(os.getenv("EPOCHS", "50")),
    fp16=torch.cuda.is_available(),
    save_strategy="epoch",
    evaluation_strategy="epoch",
    logging_steps=50,
    report_to=["tensorboard"],
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    data_collator=DefaultDataCollator(),
)

trainer.train()
trainer.save_model(OUTPUT_DIR)
processor.save_pretrained(OUTPUT_DIR)

# Swap to Deformable DETR by changing just two lines:
# MODEL_ID = "SenseTime/deformable-detr-single-scale-dc5"  # or another HF ckpt
# from transformers import DeformableDetrImageProcessor as AutoImageProcessor, DeformableDetrForObjectDetection as AutoModelForObjectDetection








