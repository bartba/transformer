import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from transformers import DetrForObjectDetection, DetrImageProcessor
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm
import matplotlib.pyplot as plt
from IPython.display import clear_output
import numpy as np

# 1. Custom COCO Dataset Class
class COCODetectionDataset(Dataset):
    def __init__(self, annotation_file, image_dir, processor, transforms=None):
        # Validate input paths
        if not os.path.exists(annotation_file):
            raise FileNotFoundError(f"Annotation file not found: {annotation_file}")
        if not os.path.exists(image_dir):
            raise FileNotFoundError(f"Image directory not found: {image_dir}")
        
        with open(annotation_file, 'r') as f:
            self.coco = json.load(f)
        self.image_dir = image_dir
        self.processor = processor
        self.transforms = transforms
        self.image_ids = [img['id'] for img in self.coco['images']]
        self.annotations = {}
        for ann in self.coco['annotations']:
            img_id = ann['image_id']
            if img_id not in self.annotations:
                self.annotations[img_id] = []
            self.annotations[img_id].append(ann)

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = next(img for img in self.coco['images'] if img['id'] == img_id)
        img_path = os.path.join(self.image_dir, img_info['file_name'])
        
        # Verify image exists
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        
        image = Image.open(img_path).convert('RGB')

        if self.transforms:
            image = self.transforms(image)

        coco_anns = {
            'image_id': img_id,
            'annotations': self.annotations.get(img_id, [])
        }
        encoding = self.processor(images=image, annotations=coco_anns, return_tensors="pt")
        
        # Keep batch dimension and add size
        encoding['size'] = torch.tensor([img_info['height'], img_info['width']])
        
        # Debug: Print encoding structure (remove after debugging)
        # print(f"Encoding keys: {list(encoding.keys())}")
        # print(f"Pixel_values shape: {encoding['pixel_values'].shape}")
        # print(f"Labels: {encoding['labels']}")
        
        return encoding

# 2. Collate Function for Padding
def collate_fn(batch, processor):
    # Extract pixel_values, labels, and sizes from batch
    pixel_values = [item['pixel_values'] for item in batch]
    labels = [item['labels'] for item in batch]
    sizes = torch.stack([item['size'] for item in batch])
    
    # Pad pixel_values and generate pixel_mask
    padded_batch = processor.pad(
        pixel_values,
        return_pixel_mask=True,
        return_tensors="pt"
    )
    
    # Debug: Print padded batch structure (remove after debugging)
    # print(f"Padded batch keys: {list(padded_batch.keys())}")
    # print(f"Padded pixel_values shape: {padded_batch['pixel_values'].shape}")
    
    # Return batch with padded pixel_values, pixel_mask, labels, and sizes
    return {
        'pixel_values': padded_batch['pixel_values'],
        'pixel_mask': padded_batch['pixel_mask'],
        'labels': labels,
        'size': sizes
    }

# 3. Initialize Accelerator, Model, and Optimizer
accelerator = Accelerator()
device = accelerator.device
device_lr = 1e-4
batch_size = 2
num_epochs = 10
patience = 3

processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
optimizer = AdamW(model.parameters(), lr=device_lr)

# 4. Load COCO Dataset
# Update these paths to your actual COCO dataset paths
image_dir = "/path/to/coco/train2017"  # e.g., "/data/coco/train2017"
train_annotation_file = "/path/to/coco/annotations/instances_train2017.json"  # e.g., "/data/coco/annotations/instances_train2017.json"
val_annotation_file = "/path/to/coco/annotations/instances_val2017.json"  # e.g., "/data/coco/annotations/instances_val2017.json"

train_transforms = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
])

train_dataset = COCODetectionDataset(
    annotation_file=train_annotation_file,
    image_dir=image_dir,
    processor=processor,
    transforms=train_transforms
)
val_dataset = COCODetectionDataset(
    annotation_file=val_annotation_file,
    image_dir=image_dir.replace("train2017", "val2017"),
    processor=processor,
    transforms=None
)

train_loader = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True, 
    collate_fn=lambda batch: collate_fn(batch, processor)
)
val_loader = DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False, 
    collate_fn=lambda batch: collate_fn(batch, processor)
)

train_loader, val_loader, model, optimizer = accelerator.prepare(
    train_loader, val_loader, model, optimizer
)

# 5. Training Function
def train_one_epoch(model, loader, optimizer, device, epoch, accelerator):
    model.train()
    running_loss = 0.0
    for batch in tqdm(loader, desc=f"Epoch {epoch}"):
        pixel_values = batch['pixel_values'].to(device)
        labels = [{k: v.to(device) for k, v in t.items()} for t in batch['labels']]
        
        # Debug: Print batch structure (remove after debugging)
        # print(f"Batch keys: {list(batch.keys())}")
        # print(f"Pixel_values shape: {batch['pixel_values'].shape}")
        # print(f"Labels sample: {labels[0]}")
        
        outputs = model(pixel_values=pixel_values, labels=labels)
        loss = outputs.loss
        
        optimizer.zero_grad()
        accelerator.backward(loss)
        optimizer.step()
        
        running_loss += loss.item()
    
    avg_loss = running_loss / len(loader)
    print(f"Epoch: {epoch}, Train Loss: {avg_loss:.4f}")
    return avg_loss

# 6. Evaluation Function
@torch.no_grad()
def evaluate(model, loader, device, processor, threshold=0.5):
    model.eval()
    total_loss = 0.0
    all_results = []
    for batch in loader:
        pixel_values = batch['pixel_values'].to(device)
        labels = [{k: v.to(device) for k, v in t.items()} for t in batch['labels']]
        target_sizes = batch['size'].to(device)
        
        outputs = model(pixel_values=pixel_values, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        
        results = processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=threshold
        )
        all_results.extend(results)
    
    avg_loss = total_loss / len(loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    return avg_loss, all_results

# 7. Training Loop with Visualization and Early Stopping
train_losses = []
val_losses = []
best_val_loss = float('inf')
epochs_no_improve = 0

for epoch in range(1, num_epochs + 1):
    train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch, accelerator)
    val_loss, detections = evaluate(model, val_loader, device, processor)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    clear_output(wait=True)
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training and Validation Loss")
    plt.show()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        accelerator.wait_for_everyone()
        unwrapped_model = accelerator.unwrap_model(model)
        torch.save(unwrapped_model.state_dict(), "detr_finetuned.pth")
    else:
        epochs_no_improve += 1
        print(f"No improvement for {epochs_no_improve} epoch(s).")
        if epochs_no_improve >= patience:
            print(f"Early stopping at epoch {epoch}. Best val loss: {best_val_loss:.4f}")
            break

# 8. Save Final Model
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
torch.save(unwrapped_model.state_dict(), "detr_finetuned_final.pth")
